{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"architecture_search_v2_withGenerator.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"3RfC0pSlMykB","colab_type":"code","colab":{}},"source":["!pip3 install sentencepiece\n","!pip3 install tf-sentencepiece\n","!pip3 install matplotlib\n","#!pip3 install tqdm\n","#!pip install tensorflow==1.13.1"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dAn50z4pM-ly","colab_type":"code","colab":{}},"source":["import os, sys\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","gitDir = \"/content/gdrive/My Drive/nlp/\"\n","os.chdir(gitDir + \"data/\")\n","print(os.listdir(\".\"))\n","\n","#sys.path.insert(0,gitDir + \"data\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fYjbAuN7M8Ox","colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","print(tf.__version__)\n","import tensorflow_hub as hub\n","import numpy as np\n","import tf_sentencepiece\n","\n","import load_dataset\n","from tensorflow.keras.layers import *\n","from tensorflow.keras.models import *\n","\n","import glob, time, datetime\n","from tqdm import tqdm_notebook as tqdm\n","\n","from sklearn.metrics import f1_score\n","\n","print(\"Version: \", tf.__version__)\n","print(\"Eager mode: \", tf.executing_eagerly())\n","print(\"Hub version: \", hub.__version__)\n","print(\"GPU is\", \"available\" if tf.test.is_gpu_available() else \"NOT AVAILABLE\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"V1XTZNORNX2S","colab_type":"text"},"source":["## create graph"]},{"cell_type":"code","metadata":{"id":"NqT_TnJwYIPd","colab_type":"code","colab":{}},"source":["class Model:\n","    def __init__(self, data_X, data_Y, architecture, xling=None):\n","        self.n_class = 39\n","        self.architecture = architecture\n","        if xling is None:\n","          print(\"Downloading xling...\")\n","          self.xling = hub.Module(\"https://tfhub.dev/google/universal-sentence-encoder-xling-many/1\", trainable=False) \n","        else:\n","          self.xling = xling\n","        self.data_X = data_X\n","        self.data_Y = data_Y\n","        self.create_architecture(data_X, data_Y)\n","        \n","        \n","    def create_architecture(self, data_X, data_Y):\n","        #y_hot = tf.one_hot(data_Y, depth=self.n_class)\n","        self.logits = self.forward(data_X)\n","        \n","        self.loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=data_Y, logits=self.logits))\n","        self.train_op = tf.train.AdamOptimizer().minimize(self.loss)\n","\n","        self.predictions = tf.argmax(self.logits, 1)\n","        self.labels = data_Y\n","        #self.acc, self.acc_op = tf.metrics.accuracy(labels=data_Y, predictions=self.predictions)\n","        \n","        #a = tf.cast(self.predictions, tf.float64)\n","        self.accuracy = tf.reduce_mean( tf.cast(tf.equal(self.predictions, tf.cast(data_Y, tf.int64)), tf.float32) )\n","        \n","    def forward(self, X):\n","        output = self.xling(X)\n","        \n","        for x in self.architecture:\n","          if x == \"bn\":\n","            output = tf.layers.batch_normalization(output, training=True)\n","          elif x == \"relu\" or x == \"r\":\n","            output = tf.nn.relu(output)\n","          elif x == \"dropout\" or x == \"d\":\n","            output = tf.layers.dropout(output)\n","          else:\n","            output = tf.layers.dense(output, x)\n","            \n","        output = tf.layers.dense(output, self.n_class, name=\"final_output_prediction\")\n","        \n","        return output"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xk053cQYVija","colab_type":"text"},"source":["# train"]},{"cell_type":"code","metadata":{"id":"NiYScoQoQICi","colab_type":"code","colab":{}},"source":["def trainModel(model, params):\n","\n","  init_op = tf.group([tf.local_variables_initializer(), tf.global_variables_initializer(), tf.tables_initializer()])\n","  sess = tf.Session()\n","  sess.run(init_op)\n","\n","  loss_hist, acc_hist, val_loss_hist, val_acc_hist = [], [], [], []\n","  loss_hist_epoch, acc_hist_epoch, val_loss_hist_epoch, val_acc_hist_epoch, f1_train_epoch, f1_val_epoch = [], [], [], [], [], []\n","  train_predictions, train_labels, val_predictions, val_labels = [], [], [], []\n","\n","  for epoch in tqdm(range(params[\"epochs\"])):\n","    #print('\\nEpoch: {}'.format(epoch + 1))\n","    train_loss, train_accuracy = 0, 0\n","    val_loss, val_accuracy = 0, 0  \n","    counter = 0\n","\n","    sess.run(train_iterator)\n","\n","    try:\n","      with tqdm(total = params[\"trainexamples\"]) as pbar:\n","        while True:\n","          _, a, l, predictions, labels = sess.run([model.train_op, model.accuracy, model.loss, model.predictions, model.labels])\n","          #print(a,l)\n","          \n","          if l >0 and l < 15:\n","            pass\n","          else:\n","            print(l)\n","            print(counter)\n","            print(sess.run(model.data_X))\n","            #print(tf.print(model.data_Y))\n","                  \n","                  \n","          train_loss += l\n","          train_accuracy += a\n","          loss_hist.append(l)\n","          acc_hist.append(a)\n","          pbar.set_postfix_str((l, a))\n","          pbar.update(params[\"batchSize\"])\n","          \n","          train_predictions.extend(predictions)\n","          train_labels.extend(labels)\n","          \n","          counter += 1\n","    except tf.errors.OutOfRangeError:\n","       pass\n","       #print(\"\\tfinished after\", counter, \"batches.\")\n","\n","\n","    loss_hist_epoch.append(train_loss / counter)\n","    acc_hist_epoch.append(train_accuracy / counter)\n","    train_f1 = f1_score(train_labels, train_predictions, average=\"micro\")\n","    f1_train_epoch.append(train_f1)\n","    #print('\\nEpoch: {}'.format(epoch + 1))\n","    \n","\n","\n","    # Validation\n","    counter = 0\n","    sess.run(val_iterator) \n","    try:\n","      with tqdm(total = params[\"trainexamples\"]) as pbar:\n","        while True:\n","          a, l, p, labels = sess.run([model.accuracy, model.loss, model.predictions, model.labels])\n","          val_loss += l\n","          val_accuracy += a\n","          val_loss_hist.append(l)\n","          val_acc_hist.append(a)\n","          pbar.set_postfix_str((l, a))\n","          pbar.update(params[\"batchSize\"])\n","          \n","          val_predictions.extend(p)\n","          val_labels.extend(labels)\n","          \n","          counter += 1\n","    except tf.errors.OutOfRangeError:\n","       pass\n","       #print(\"\\tfinished after\", counter, \"batches.\")\n","\n","\n","    val_loss_hist_epoch.append(val_loss / counter)\n","    val_acc_hist_epoch.append(val_accuracy / counter)\n","    val_f1 = f1_score(val_labels, val_predictions, average=\"micro\")\n","    f1_val_epoch.append(val_f1)\n","    print('\\tEpoch {}: train_loss: {:.4f}, train_acc: {:.4f}, train_micro-f1: {:.4f} || val_loss: {:.4f}, val_acc: {:.4f}, val_micro-f1: {:.4f}'.format(epoch + 1, loss_hist_epoch[-1], acc_hist_epoch[-1], train_f1, val_loss_hist_epoch[-1], val_acc_hist_epoch[-1], val_f1))\n","    \n","  \n","  sess.close()\n","  return loss_hist_epoch, acc_hist_epoch, val_loss_hist_epoch , val_acc_hist_epoch, f1_train_epoch, f1_val_epoch"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"J5N1d6XdT2oQ","colab_type":"code","colab":{}},"source":["import matplotlib.pyplot as plt\n","import datetime, os\n","\n","def plotResults(x1, label1, x2, label2, title, path, architecture):\n","  plt.plot(x1, label=label1)\n","  plt.plot(x2, label=label2)\n","  plt.legend()\n","  plt.title(title)\n","  #plt.ylim(0,1)\n","  figpath = path + title + \".png\"\n","  #figpath = figpath.replace()\n","  #print(figpath)\n","  plt.savefig(figpath)\n","  plt.show()\n","  \n","  np.save(path + str(architecture) + label1 + \".npy\", x1)\n","  np.save(path + str(architecture) + label2 + \".npy\", x2)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"WllKcR_ZbEPs","colab_type":"code","colab":{}},"source":["params = {}\n","params[\"trainData\"] = \"US\"\n","params[\"testData\"] = \"DE\"\n","params[\"epochs\"] = 15\n","params[\"batchSize\"] = 512\n","params[\"trainexamples\"] = 1000 * 100\n","params[\"savelog\"] = True\n","\n","\n","#done [False,50], [False,150], [False,150,50],  \n","architectures = [ [False] ]\n","\n","path=\"./\"\n","if params[\"savelog\"]==True:\n","  path = '/content/gdrive/My Drive/nlp/blobs/' + datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\") + \"/\"\n","  os.mkdir(path)\n","  print(\"saving to:\", path)\n","\n","  f = open(path + \"info.txt\", \"w\")\n","  f.write(str(params)+\"\\n architectures: \"+str(architectures))\n","  f.close()\n","  \n","loss_hist_epoch, acc_hist_epoch, val_loss_hist_epoch, val_acc_hist_epoch = [],[],[],[]\n","\n","for a in architectures:\n","  tf.reset_default_graph()\n","  dataset_train = load_dataset.getData(params[\"trainData\"], shuffle=True, batchsize=params[\"batchSize\"])\n","  dataset_val = load_dataset.getData(params[\"testData\"], shuffle=False, batchsize=params[\"batchSize\"])\n","  \n","  dataset_train = dataset_train.take(int(params[\"trainexamples\"]/params[\"batchSize\"]))\n","  dataset_val = dataset_val.take(int(params[\"trainexamples\"]/params[\"batchSize\"]))\n","  \n","  iterator = tf.data.Iterator.from_structure(dataset_train.output_types, dataset_train.output_shapes)\n","  train_iterator = iterator.make_initializer(dataset_train)\n","  val_iterator = iterator.make_initializer(dataset_val)\n","  text_input, label = iterator.get_next()\n","  \n","  xling = hub.Module(\"https://tfhub.dev/google/universal-sentence-encoder-xling/en-de/1\", trainable=a[0])\n","  model = Model(text_input, label, a[1:], xling)\n","  print(str(a))\n","  %time loss_hist_epoch, acc_hist_epoch, val_loss_hist_epoch, val_acc_hist_epoch, f1_train_epoch, f1_val_epoch = trainModel(model, params)\n","  print(\"\\t\", loss_hist_epoch, acc_hist_epoch, \"\\n\\t\", val_loss_hist_epoch, val_acc_hist_epoch)\n","  plotResults(loss_hist_epoch, \"train_loss\", val_loss_hist_epoch, \"val_loss\", str(a)+\" loss\", path, a)\n","  plotResults(acc_hist_epoch, \"acc_train\", val_acc_hist_epoch, \"acc_val\", str(a)+ \" acc\", path, a)\n","  plotResults(f1_train_epoch, \"f1_train\", f1_val_epoch, \"f1_val\", str(a) + \" f1\", path, a)\n","  \n","  tf.reset_default_graph()\n","  del model, xling, dataset_train, dataset_val, iterator,train_iterator,val_iterator,text_input, label\n","  tf.reset_default_graph()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5D1vs2MtuWKF","colab_type":"code","colab":{}},"source":["for f in glob.glob(path+\"*loss.npy\"):\n","  x = np.load(f)\n","  plt.plot(x, label=f[len(path):-4])\n","plt.legend()\n","plt.ylim(0,5)\n","plt.savefig(path+\"allloss.png\")\n","plt.show()\n","\n","for f in glob.glob(path+\"*acc*.npy\"):\n","  x = np.load(f)\n","  plt.plot(x, label=f[len(path):-4])\n","plt.legend()\n","plt.ylim(0,1)\n","plt.savefig(path+\"allacc.png\")\n","plt.show()\n","\n","for f in glob.glob(path+\"*f1*.npy\"):\n","  x = np.load(f)\n","  plt.plot(x, label=f[len(path):-4])\n","plt.legend()\n","plt.ylim(0,1)\n","plt.savefig(path+\"allf1.png\")\n","plt.show()"],"execution_count":0,"outputs":[]}]}