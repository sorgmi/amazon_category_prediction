{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"training_savemodel.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"oBjEDM_NxdOm","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":136},"outputId":"b0bcdfd5-a28a-4728-fe19-634d763b826f","executionInfo":{"status":"ok","timestamp":1560526527075,"user_tz":-120,"elapsed":8527,"user":{"displayName":"Michael Sorg","photoUrl":"https://lh6.googleusercontent.com/-SEgKX5ssgFM/AAAAAAAAAAI/AAAAAAAAA7g/ZczybXIXNlM/s64/photo.jpg","userId":"02128442314109013475"}}},"source":["%load_ext autoreload\n","%autoreload 2\n","\n","!pip3 install --quiet tensorflow-gpu==1.13.1\n","!pip3 install --quiet tensorflow-hub\n","!pip3 install --quiet tf-sentencepiece\n","\n","import tensorflow as tf\n","print(\"Version: \", tf.__version__)\n","print(\"Eager mode: \", tf.executing_eagerly())\n","print(\"GPU is\", \"available\" if tf.test.is_gpu_available() else \"NOT AVAILABLE\")\n","\n","import os, sys,datetime\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","gitDir = \"/content/gdrive/My Drive/nlp/\"\n","os.chdir(gitDir + \"training/\")\n","os.chdir(gitDir)\n","print(os.listdir(\".\"))\n","\n","#sys.path.insert(0,gitDir + \"data\")\n","\n","#import training.train as t"],"execution_count":3,"outputs":[{"output_type":"stream","text":["The autoreload extension is already loaded. To reload it, use:\n","  %reload_ext autoreload\n","Version:  1.13.1\n","Eager mode:  False\n","GPU is available\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","['.git', 'README.md', 'data', 'presentations', '.idea', 'training', 'blobs', '.gitignore', 'serverblobs', 'template.py']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"kbKwmsCsxnXW","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":476},"outputId":"c5d4823d-357f-4d3a-8b6a-1cbd65c4e4cf","executionInfo":{"status":"ok","timestamp":1560527673949,"user_tz":-120,"elapsed":42261,"user":{"displayName":"Michael Sorg","photoUrl":"https://lh6.googleusercontent.com/-SEgKX5ssgFM/AAAAAAAAAAI/AAAAAAAAA7g/ZczybXIXNlM/s64/photo.jpg","userId":"02128442314109013475"}}},"source":["%%time\n","\n","\n","import tensorflow as tf\n","import tensorflow_hub as hub\n","import numpy as np\n","import tf_sentencepiece\n","\n","from data import load_dataset\n","from tensorflow.keras.layers import *\n","from tensorflow.keras.models import *\n","\n","import glob, time, datetime, os\n","import matplotlib.pyplot as plt\n","#from tqdm import tqdm_notebook as tqdm\n","from tqdm import tqdm as tqdm\n","\n","from sklearn.metrics import f1_score\n","\n","params = {}\n","params[\"trainData\"] = \"US\"\n","params[\"testData\"] = \"DE\"\n","params[\"epochs\"] = 1\n","params[\"batchSize\"] = 512\n","params[\"optimizer\"] = tf.train.AdamOptimizer()\n","params[\"trainexamples\"] = 1000 * 1\n","params[\"architecture\"] = [False]\n","params[\"f1modus\"] = \"micro\"\n","params[\"savelog\"] = False\n","params[\"path\"] = \"blobs/\" + datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\") + \"_myTitle/\"\n","#params[\"path\"] = \"blobs/test/\"\n","#params[\"pathToCache\"] = \"../data/\"\n","\n","\n","class Model:\n","    def __init__(self, data_X, data_Y, params):\n","        self.params = params\n","        self.n_class = 39\n","        self.architecture = params[\"architecture\"][1:]\n","        #print(\"Downloading xling...\")\n","        self.xling = hub.Module(\"https://tfhub.dev/google/universal-sentence-encoder-xling-many/1\", trainable=params[\"architecture\"][0])\n","        self.data_X = data_X\n","        self.data_Y = data_Y\n","        self.create_architecture(data_X, data_Y)\n","\n","    def create_architecture(self, data_X, data_Y):\n","        # y_hot = tf.one_hot(data_Y, depth=self.n_class)\n","        self.logits = self.forward(data_X)\n","\n","        self.loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=data_Y, logits=self.logits))\n","        self.train_op = self.params[\"optimizer\"].minimize(self.loss)\n","\n","        self.predictions = tf.argmax(self.logits, 1)\n","        self.labels = data_Y\n","        # self.acc, self.acc_op = tf.metrics.accuracy(labels=data_Y, predictions=self.predictions)\n","\n","        # a = tf.cast(self.predictions, tf.float64)\n","        self.accuracy = tf.reduce_mean(tf.cast(tf.equal(self.predictions, tf.cast(data_Y, tf.int64)), tf.float32))\n","\n","    def forward(self, X):\n","        output = self.xling(X)\n","\n","        for x in self.architecture:\n","            if x == \"bn\":\n","                output = tf.layers.batch_normalization(output, training=True)\n","            elif x == \"relu\" or x == \"r\":\n","                output = tf.nn.relu(output)\n","            elif x == \"dropout\" or x == \"d\":\n","                output = tf.layers.dropout(output)\n","            else:\n","                output = tf.layers.dense(output, x)\n","\n","        output = tf.layers.dense(output, self.n_class, name=\"final_output_prediction\")\n","\n","        return output\n","    \n","\n","def trainModel(p):\n","    # init default params\n","    params = {}\n","    #params[\"trainData\"] = \"US\"\n","    #params[\"testData\"] = \"DE\"\n","    params[\"epochs\"] = 15\n","    params[\"batchSize\"] = 512\n","    params[\"optimizer\"] = tf.train.AdamOptimizer()\n","    params[\"trainexamples\"] = 1000 * 100\n","    #params[\"architecture\"] = [False]\n","    params[\"f1modus\"] = \"micro\"\n","    params[\"savelog\"] = True\n","    params[\"path\"] = \"blobs/\" + datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\") + \"/\"\n","    params[\"pathToCache\"] = \"data/\"\n","\n","    params.update(p)  # overwrite default parameter with passed parameter\n","\n","    if params[\"savelog\"] == True:\n","        '''\n","        if params[\"path\"] is None:\n","            path = '/content/gdrive/My Drive/nlp/blobs/' + datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\") + \"/\"\n","            os.mkdir(path)\n","            params[\"path\"] = path\n","        else:\n","            path = params[\"path\"]\n","        '''\n","\n","        print(\"saving to:\", params[\"path\"])\n","        if os.path.exists(params[\"path\"]) is False:\n","            os.mkdir(params[\"path\"])\n","        f = open(params[\"path\"] + \"info.txt\", \"w\")\n","        for k in params:\n","            f.write(k + \": \" + str(params[k]) + \"\\n\")\n","        f.close()\n","\n","\n","    tf.reset_default_graph()\n","    dataset_train = load_dataset.getData(params[\"trainData\"], shuffle=True, batchsize=params[\"batchSize\"], pathToCache=params[\"pathToCache\"])\n","    dataset_val = load_dataset.getData(params[\"testData\"], shuffle=False, batchsize=params[\"batchSize\"], pathToCache=params[\"pathToCache\"])\n","\n","    if params[\"trainexamples\"] is not None:\n","        dataset_train = dataset_train.take(int(params[\"trainexamples\"] / params[\"batchSize\"]))\n","        dataset_val = dataset_val.take(int(params[\"trainexamples\"] / params[\"batchSize\"]))\n","\n","    iterator = tf.data.Iterator.from_structure(dataset_train.output_types, dataset_train.output_shapes)\n","    train_iterator = iterator.make_initializer(dataset_train)\n","    val_iterator = iterator.make_initializer(dataset_val)\n","    text_input, label = iterator.get_next()\n","\n","    model = Model(text_input, label, params)\n","\n","    init_op = tf.group([tf.local_variables_initializer(), tf.global_variables_initializer(), tf.tables_initializer()])\n","    sess = tf.Session()\n","    sess.run(init_op)\n","\n","    loss_hist, acc_hist, val_loss_hist, val_acc_hist = [], [], [], []\n","    loss_hist_epoch, acc_hist_epoch, val_loss_hist_epoch, val_acc_hist_epoch, f1_train_epoch, f1_val_epoch = [], [], [], [], [], []\n","    train_predictions, train_labels, val_predictions, val_labels = [], [], [], []\n","\n","    startTime = time.time()\n","    for epoch in tqdm(range(params[\"epochs\"])):\n","        # print('\\nEpoch: {}'.format(epoch + 1))\n","        train_loss, train_accuracy = 0, 0\n","        val_loss, val_accuracy = 0, 0\n","        counter = 0\n","\n","        sess.run(train_iterator)\n","\n","        try:\n","            with tqdm(total=params[\"trainexamples\"]) as pbar:\n","                while True:\n","                    _, a, l, predictions, labels = sess.run(\n","                        [model.train_op, model.accuracy, model.loss, model.predictions, model.labels])\n","                    # print(a,l)\n","\n","                    '''\n","                    if l > 0 and l < 15:\n","                        pass\n","                    else:\n","                        print(l)\n","                        print(counter)\n","                        print(sess.run(model.data_X))\n","                        # print(tf.print(model.data_Y))\n","                    '''\n","\n","                    train_loss += l\n","                    train_accuracy += a\n","                    loss_hist.append(l)\n","                    acc_hist.append(a)\n","                    pbar.set_postfix_str((l, a))\n","                    pbar.update(params[\"batchSize\"])\n","\n","                    train_predictions.extend(predictions)\n","                    train_labels.extend(labels)\n","\n","                    counter += 1\n","        except tf.errors.OutOfRangeError:\n","            pass\n","            # print(\"\\tfinished after\", counter, \"batches.\")\n","\n","        loss_hist_epoch.append(train_loss / counter)\n","        acc_hist_epoch.append(train_accuracy / counter)\n","        train_f1 = f1_score(train_labels, train_predictions, average=params[\"f1modus\"])\n","        f1_train_epoch.append(train_f1)\n","        # print('\\nEpoch: {}'.format(epoch + 1))\n","\n","        # Validation\n","        counter = 0\n","        sess.run(val_iterator)\n","        try:\n","            with tqdm(total=params[\"trainexamples\"]) as pbar:\n","                while True:\n","                    a, l, p, labels = sess.run([model.accuracy, model.loss, model.predictions, model.labels])\n","                    val_loss += l\n","                    val_accuracy += a\n","                    val_loss_hist.append(l)\n","                    val_acc_hist.append(a)\n","                    pbar.set_postfix_str((l, a))\n","                    pbar.update(params[\"batchSize\"])\n","\n","                    val_predictions.extend(p)\n","                    val_labels.extend(labels)\n","\n","                    counter += 1\n","        except tf.errors.OutOfRangeError:\n","            pass\n","            # print(\"\\tfinished after\", counter, \"batches.\")\n","\n","        val_loss_hist_epoch.append(val_loss / counter)\n","        val_acc_hist_epoch.append(val_accuracy / counter)\n","        val_f1 = f1_score(val_labels, val_predictions, average=params[\"f1modus\"])\n","        f1_val_epoch.append(val_f1)\n","        print(\n","            '\\n\\tEpoch {}: train_loss: {:.4f}, train_acc: {:.4f}, train_micro-f1: {:.4f} || val_loss: {:.4f}, val_acc: {:.4f}, val_micro-f1: {:.4f}'.format(\n","                epoch + 1, loss_hist_epoch[-1], acc_hist_epoch[-1], train_f1, val_loss_hist_epoch[-1],\n","                val_acc_hist_epoch[-1], val_f1))\n","\n","\n","        # Epoch finished - update and save results\n","        trainingTime = time.time() - startTime\n","        result = {}\n","        result[\"loss_hist_epoch\"] = loss_hist_epoch\n","        result[\"acc_hist_epoch\"] = acc_hist_epoch\n","        result[\"val_loss_hist_epoch\"] = val_loss_hist_epoch\n","        result[\"val_acc_hist_epoch\"] = val_acc_hist_epoch\n","        result[\"f1_train_epoch\"] = f1_train_epoch\n","        result[\"f1_val_epoch\"] = f1_val_epoch\n","        result[\"loss_hist_epoch\"] = loss_hist_epoch\n","        result[\"loss_hist_epoch\"] = loss_hist_epoch\n","        result[\"train_time_seconds\"] = trainingTime\n","        result[\"train_time_minutes\"] = trainingTime / 60\n","\n","        if params[\"savelog\"] == True:\n","            f = open(params[\"path\"] + \"result.txt\", \"w\")\n","            for k in result:\n","                f.write(k + \": \" + str(result[k]) + \"\\n\")\n","            f.close()\n","\n","            # save plots\n","            path = params[\"path\"]\n","            print(\"saving results to:\", path)\n","            a = params[\"architecture\"]\n","            plotResults(loss_hist_epoch, \"train_loss\", val_loss_hist_epoch, \"val_loss\", str(a) + \" loss\", path, a)\n","            plotResults(acc_hist_epoch, \"acc_train\", val_acc_hist_epoch, \"acc_val\", str(a) + \" acc\", path, a)\n","            plotResults(f1_train_epoch, \"f1_train\", f1_val_epoch, \"f1_val\", str(a) + \" f1\", path, a)\n","\n","    \n","    \n","    \n","    \n","    #plotAll(path)\n","    #sess.close()\n","\n","    return result, sess  \n","  \n","  \n","result, sess = trainModel(params)"],"execution_count":19,"outputs":[{"output_type":"stream","text":["data/cache/amazon_reviews_multilingual_US_v1_00.tsv.shuffled.csv already exists. Using cached data\n","data/cache/amazon_reviews_multilingual_DE_v1_00.tsv.shuffled.csv already exists. Using cached data\n","INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"],"name":"stdout"},{"output_type":"stream","text":["I0614 15:54:18.041616 140022028396416 saver.py:1483] Saver not created because there are no variables in the graph to restore\n","\n","  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n","\n","  0%|          | 0/1000 [00:00<?, ?it/s]\u001b[A\u001b[A\n","\n","  0%|          | 0/1000 [00:02<?, ?it/s, (3.6507633, 0.037109375)]\u001b[A\u001b[A\n","\n"," 51%|█████     | 512/1000 [00:02<00:02, 184.65it/s, (3.6507633, 0.037109375)]\u001b[A\u001b[A\n","\n","\u001b[A\u001b[A\n","\n","  0%|          | 0/1000 [00:00<?, ?it/s]\u001b[A\u001b[A\n","\n","  0%|          | 0/1000 [00:02<?, ?it/s, (3.6521745, 0.044921875)]\u001b[A\u001b[A\n","\n"," 51%|█████     | 512/1000 [00:02<00:02, 205.19it/s, (3.6521745, 0.044921875)]\u001b[A\u001b[A\n","\n","\u001b[A\u001b[A\n","100%|██████████| 1/1 [00:07<00:00,  7.70s/it]\u001b[A\n","\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["\n","\tEpoch 1: train_loss: 3.6508, train_acc: 0.0371, train_micro-f1: 0.0371 || val_loss: 3.6522, val_acc: 0.0449, val_micro-f1: 0.0449\n","CPU times: user 39.5 s, sys: 2.06 s, total: 41.6 s\n","Wall time: 41.3 s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"HAHTIwBDoLpU","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":204},"outputId":"830086a0-ed28-47b4-db7c-be8449334ce8","executionInfo":{"status":"ok","timestamp":1560527691568,"user_tz":-120,"elapsed":57798,"user":{"displayName":"Michael Sorg","photoUrl":"https://lh6.googleusercontent.com/-SEgKX5ssgFM/AAAAAAAAAAI/AAAAAAAAA7g/ZczybXIXNlM/s64/photo.jpg","userId":"02128442314109013475"}}},"source":["!rm -rf /content/SavedModel\n","\n","builder = tf.saved_model.builder.SavedModelBuilder('/content/SavedModel/')\n","\n","builder.add_meta_graph_and_variables(sess,\n","                                       [tf.saved_model.tag_constants.TRAINING],\n","                                       signature_def_map=None,\n","                                       assets_collection=None)\n","builder.save()  \n","\n","!ls -alh /content/SavedModel"],"execution_count":20,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:No assets to save.\n"],"name":"stdout"},{"output_type":"stream","text":["I0614 15:54:35.977722 140022028396416 builder_impl.py:629] No assets to save.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:No assets to write.\n"],"name":"stdout"},{"output_type":"stream","text":["I0614 15:54:35.980166 140022028396416 builder_impl.py:449] No assets to write.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:SavedModel written to: /content/SavedModel/saved_model.pb\n"],"name":"stdout"},{"output_type":"stream","text":["I0614 15:54:50.630439 140022028396416 builder_impl.py:414] SavedModel written to: /content/SavedModel/saved_model.pb\n"],"name":"stderr"},{"output_type":"stream","text":["total 161M\n","drwxr-xr-x 3 root root 4.0K Jun 14 15:54 .\n","drwxr-xr-x 1 root root 4.0K Jun 14 15:54 ..\n","-rw-r--r-- 1 root root 161M Jun 14 15:54 saved_model.pb\n","drwxr-xr-x 2 root root 4.0K Jun 14 15:54 variables\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8RuxY2DzoXBp","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3CmVQiwXqR6m","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":187},"outputId":"e39cc1cd-39e7-4e0c-e8f6-990942723f5e","executionInfo":{"status":"ok","timestamp":1560527916892,"user_tz":-120,"elapsed":24767,"user":{"displayName":"Michael Sorg","photoUrl":"https://lh6.googleusercontent.com/-SEgKX5ssgFM/AAAAAAAAAAI/AAAAAAAAA7g/ZczybXIXNlM/s64/photo.jpg","userId":"02128442314109013475"}}},"source":["!rm -rf /content/Saver\n","saver = tf.train.Saver()\n","saver.save(sess, '/content/Saver/iter', global_step=23)\n","saver.save(sess, '/content/Saver/iter', global_step=66)\n","!ls -alh /content/Saver/"],"execution_count":25,"outputs":[{"output_type":"stream","text":["total 1.2G\n","drwxr-xr-x 2 root root 4.0K Jun 14 15:58 .\n","drwxr-xr-x 1 root root 4.0K Jun 14 15:58 ..\n","-rw-r--r-- 1 root root  154 Jun 14 15:58 checkpoint\n","-rw-r--r-- 1 root root 444M Jun 14 15:58 iter-23.data-00000-of-00001\n","-rw-r--r-- 1 root root  46K Jun 14 15:58 iter-23.index\n","-rw-r--r-- 1 root root 164M Jun 14 15:58 iter-23.meta\n","-rw-r--r-- 1 root root 444M Jun 14 15:58 iter-66.data-00000-of-00001\n","-rw-r--r-- 1 root root  46K Jun 14 15:58 iter-66.index\n","-rw-r--r-- 1 root root 164M Jun 14 15:58 iter-66.meta\n"],"name":"stdout"}]}]}